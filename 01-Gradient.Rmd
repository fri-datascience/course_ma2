# Gradient-based methods {#gradient}

```{r, echo = FALSE}
togs   <- T
tog_ex <- T
```

<style>
.fold-btn { 
  float: right; 
  margin: 5px 5px 0 0;
}
.fold { 
  border: 1px solid black;
  min-height: 40px;
}
</style>

<script type="text/javascript">
$(document).ready(function() {
  $folds = $(".fold");
  $folds.wrapInner("<div class=\"fold-blck\">"); // wrap a div container around content
  $folds.prepend("<button class=\"fold-btn\">Unfold Solution</button>");  // add a button
  $(".fold-blck").toggle();  // fold all blocks
  $(".fold-btn").on("click", function() {  // add onClick event
    $(this).text($(this).text() === "Fold Solution" ? "Unfold Solution" : "Fold Solution");  // if the text equals "Fold", change it to "Unfold"or else to "Fold" 
    $(this).next(".fold-blck").toggle("linear");  // "swing" is the default easing function. This can be further customized in its speed or the overall animation itself.
  })
});
</script>


## Theoretical problems

```{exercise, name = "Classes of functions"}
Let $L, \alpha, \beta>0, A, B\subseteq \mathbb{R}$ and define:

* $\mathcal{L}( L,A,B)$ as the set of all $L$-Lipschitz functions $A \to B$.
* $\mathcal{S}(\beta, A,B)$ as the set of all $\beta$-smooth functions $A \to B$.
* $\mathcal{C}(\alpha, A,B)$ as the set of all  $\alpha$-strongly convex functions $A \to B$.

Questions:
  
a. For which $B$ are $\mathcal{L}( L,A,B)$, $\mathcal{S}(\beta, A,B)$, and $\mathcal{C}(\alpha, A,B)$ vector spaces?
b. Are $\mathcal{L}( L,A,B)$, $\mathcal{S}(\beta, A,B)$, and $\mathcal{C}(\alpha, A,B)$ closed for compositions, i.e., if $f,g$ are from one of the above sets with $A=\mathbb{R}$, is $f \circ g$ also from the same class? 
c. Are $\mathcal{L}( L,A,B)$, $\mathcal{S}(\beta, A,B)$, and $\mathcal{C}(\alpha, A,B)$ closed for products, i.e., if $f,g$ are from one of the above sets, is $f \cdot g$ also from the same class?
d. Are non-negative function from $\mathcal{L}( L,A,B)$, $\mathcal{S}(\beta, A,B)$, and $\mathcal{C}(\alpha, A,B)$ closed square roots, i.e., if $f>0$ is from one of the above sets, is $\sqrt{f}$ also from the same class?
e. Some of the answers to the previous questions are negative. By manipulating or restricting parameters $L, \alpha, \beta$, $A$ and the images of functions develop versions of the above statements with affirmative answers.

      For example, if $f,g\in \mathcal{L}( L,\mathbb{R},\mathbb{R})$ then  $f \circ g$ might not be in $\mathcal{L}( L,\mathbb{R},\mathbb{R})$. However $f \circ g \in  \mathcal{L}( L^2,\mathbb{R}, \mathbb{R})$.  Think about why this is true. On a similar note, if $f,g\in \mathcal{L}( L,\mathbb{R},[a,b])$ for some positive $a<b$ then $f\cdot g \in \mathcal{L}( 2Lb,\mathbb{R},[a^2,b^2])$ for some $a',b', L'$.
```



```{exercise, name = "Gradient descent"}
Let $f(x,y)=2x + e^{2x} + y^2$. Function $f$ restricted to $K=[-3,3]\times [-3,3]$ is Lipschitz, smooth and strongly convex. Find some corresponding (preferably optimal) constants $L, \alpha$ and $\beta$ on $K$. Prove that $f$ is convex. Determine the optimal learning rate $\gamma$ for the gradient descent method for this function.
```

<div class="fold">
```{solution, echo = tog_ex}
First, we compute the gradient

$$\nabla f = \begin{bmatrix}
2 + 2e^{2x}\\
2y
\end{bmatrix}$$
  
and from

$$||\nabla f|| = \sqrt{(2 + 2e^{2x})^2 + (2y)^2} \leq \sqrt{(2 + 2e^{6})^2 + 6^2} = L \text{ on } D$$
  
it follows that $f$ is Lipshitz.

Next, we compute the Hessian

$$\nabla f = \begin{bmatrix}
4e^{2x} & 0\\
0 & 2
\end{bmatrix}.$$
  
The eigenvalues $4e^{2x}$ and $2$ are always positive, so $f$ is convex. The minimum and maximum eigenvalues on $D$ are $\alpha = 4e^{-6}$ and $\beta = 4e^{6}$, so $f$ is $4e^{-6}$-strongly convex and $4e^{6}$-smooth, respectively.

It follows that the optimal learning rate is

$$\gamma = \frac{2}{\alpha + \beta} = \frac{2}{4e^{-6} + 4e^{6}}.$$

```
</div>


```{exercise, name = "Gradient descent"}
Let $f(x,y)= (x+y)^2 + (x-2y-3)^2$. 	Starting with $x_1=(0,1)$:
  
a. What is the minimal function value that can be achieved with one step of the gradient descent, i.e., find the minimum of $f(x_2)$.
b. How close to the actual minumum $x^*$ of function $f$ can we get with one step of the gradient descent, i.e., find the minimum of the distance from $x^*$ to $x_2$.
```

<div class="fold">
```{solution, echo = tog_ex}
First, we compute the gradient:
  
$$f(x, y) = x^2 + 2xy + y^2 + x^2+ 4y^2 + 9 - 4xy - 6x + 12y = 2x^2 + 5y^2 - 2xy - 6x + 12y + 9$$
  
and 

$$\nabla f = \begin{bmatrix}
4x - 2y - 6\\
10y - 2x + 12
\end{bmatrix}.$$
  
The value of the gradient at $x_1 = (0, 1)$ is $\begin{bmatrix}
-8\\
22
\end{bmatrix}$, so

$$x_2(\gamma) = x_1 - \gamma \nabla f(x_1) = \begin{bmatrix}
0\\
1
\end{bmatrix} - \gamma \begin{bmatrix}
-8\\
22
\end{bmatrix} = \begin{bmatrix}
8\gamma\\
1 - 22\gamma
\end{bmatrix}$$.

The function

$$g(\gamma) = f(8\gamma, 1 - 22\gamma)$$
$$= 128\gamma^2 + 5 + 2420\gamma^2 - 220\gamma - 16\gamma + 352 \gamma^2 - 48\gamma + 12 - 264\gamma + 9$$
$$= 2900\gamma^2 - 548\gamma + 26$$
  
is quadratic. Its derivative is $5800x - 548$ and $g$ is minimized at $\gamma^* = \frac{548}{5800} = \frac{137}{1450}$. Pluggin this into $g(\gamma)$, we get $\frac{81}{725}$ - the minimum function value that can be achieved with a single step of gradient descent.

The global minimum of $f$ is 0 at $x^* = (1, -1)$. We get the closest to $x^*$ at 

$$\gamma^* = \arg\min_\gamma \sqrt{(8\gamma - 1)^2 + (1 - 22\gamma - (-1))^2}$$
$$= \arg\min_\gamma (8\gamma - 1)^2 + (1 - 22\gamma - (-1))^2$$
$$= \arg\min_\gamma 548\gamma^2 - 104\gamma + 5.$$

The above quadratic is minimized at $\gamma^* = \frac{13}{137}$ and the minimum  achievable distance is $\frac{9}{137}$.

```
</div>

## Practical problems

Find the optimum of these functions using the methods and techniques we learned in this course:

```{exercise, name = "Minimization problem - the Ackley function"}
$$f(x_0 \cdots x_n) = -20 \exp\left(-0.2 \sqrt{\frac{1}{n} \sum_{i=1}^n x_i^2}\right) - \exp\left(\frac{1}{n} \sum_{i=1}^n cos(2\pi x_i)\right) + 20 + e$$ 

$$-32 \leq x_i \leq 32$$ 

$$\text{minimum at }f(0, \cdots, 0) = 0$$
```

```{exercise, name = "Minimization problem - an exponential function"}
$$f(x_1 \cdots x_n) = \sum_{i=1}^n e^{ix_i}i + \alpha$$ 

$$\alpha = -\sum_{i=1}^n e^{-5.12i}$$ 

$$-5.12 \leq x_i \leq 5.12$$ 

$$\text{minimum at }f(-5.12, \cdots, -5.12) = 0$$
```

```{exercise, name = "Minimization problem - the Griewank function"}
$$f(x_1 \cdots x_n) = 1 + \frac{1}{4000} \sum_{i=1}^n x^2_i - \prod_{i=1}^n cos(\frac{x_i}{\sqrt{i}})$$ 

$$-512 \leq x_i \leq 512$$ 

$$\text{minimum at }f(0, \cdots, 0) = 0$$
```

```{exercise, name = "Minimization problem - the Ridge function"}
$$f(x_1 \cdots x_n) = \sum_{i=1}^n \left(\sum_{j=1}^i x_j\right)^2$$ 

$$-64 \leq x_i \leq 64$$ 

$$\text{minimum at }f(0, \cdots, 0) = 0$$
```

```{exercise, name = "Minimization problem - the Rosenbrock function"}
$$f(x_1 \cdots x_n) = \sum_{i=1}^{n-1} (100(x_i^2 - x_{i+1})^2 + (1-x_i)^2)$$ 

$$-2.048 \leq x_i \leq 2.048$$ 

$$\text{minimum at }f(1, 1, \cdots, 1) = 0$$
```

```{exercise, name = "Minimization problem - the Whitley function"}
$$f(x_1 \cdots x_n) = \sum_{i=1}^n \sum_{j=1}^n (\frac{(100(x_i^2-x_j)^2 + (1-x_j)^2)^2}{4000} -\cos(100(x_i^2-x_j)^2 + (1-x_j)^2)+1)$$ 

$$-10.24 \leq x_i \leq 10.24$$ 

$$\text{minimum at }f(1, 1, \cdots, 1) = 0$$

```

```{exercise, name = "Minimization problem - the Matyas function"}
$$
f(x,y)= .26 (x^2 + y^2) - .48 xy
$$
$$
-10 \leq x,y \leq 10
$$
$$\text{minimum at }f(0, 0) = 0$$
```

```{exercise, name = "Minimization problem - the McCormick function"}
$$
f(x,y)= \sin(x+y)+(x-y)^2 - 1.5 x + 2.5 y + 1
$$
$$
x\in[-1.5,4], y\in[-3.4]
$$
$$\text{minimum at }f(-.54719, -1.54719) = -1.9133$$

```

```{exercise, name = "Minimization problem - the Three-Hump Camel function"}
$$
f(x,y)= 2x^2 - 1.05 x^4 + x^6/6 + xy + y^2
$$
$$
-5 \leq x,y \leq 5
$$
$$\text{minimum at }f(0,0) = 0$$
```
