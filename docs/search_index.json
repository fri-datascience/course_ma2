[
["index.html", "Mathematics 2 - exercises Preface", " Mathematics 2 - exercises Erik Štrumbelj and Žiga Virk 2021-03-21 Preface These are the exercises for the Mathematics 2 course of the Data Science Master's at University of Ljubljana, Faculty of Computer and Information Science. "],
["gradient.html", "Chapter 1 Gradient-based methods 1.1 Theoretical problems 1.2 Practical problems", " Chapter 1 Gradient-based methods .fold-btn { float: right; margin: 5px 5px 0 0; } .fold { border: 1px solid black; min-height: 40px; } 1.1 Theoretical problems Exercise 1.1 (Classes of functions) Let \\(L, \\alpha, \\beta&gt;0, A, B\\subseteq \\mathbb{R}\\) and define: \\(\\mathcal{L}( L,A,B)\\) as the set of all \\(L\\)-Lipschitz functions \\(A \\to B\\). \\(\\mathcal{S}(\\beta, A,B)\\) as the set of all \\(\\beta\\)-smooth functions \\(A \\to B\\). \\(\\mathcal{C}(\\alpha, A,B)\\) as the set of all \\(\\alpha\\)-strongly convex functions \\(A \\to B\\). Questions: For which \\(B\\) are \\(\\mathcal{L}( L,A,B)\\), \\(\\mathcal{S}(\\beta, A,B)\\), and \\(\\mathcal{C}(\\alpha, A,B)\\) vector spaces? Are \\(\\mathcal{L}( L,A,B)\\), \\(\\mathcal{S}(\\beta, A,B)\\), and \\(\\mathcal{C}(\\alpha, A,B)\\) closed for compositions, i.e., if \\(f,g\\) are from one of the above sets with \\(A=\\mathbb{R}\\), is \\(f \\circ g\\) also from the same class? Are \\(\\mathcal{L}( L,A,B)\\), \\(\\mathcal{S}(\\beta, A,B)\\), and \\(\\mathcal{C}(\\alpha, A,B)\\) closed for products, i.e., if \\(f,g\\) are from one of the above sets, is \\(f \\cdot g\\) also from the same class? Are non-negative function from \\(\\mathcal{L}( L,A,B)\\), \\(\\mathcal{S}(\\beta, A,B)\\), and \\(\\mathcal{C}(\\alpha, A,B)\\) closed square roots, i.e., if \\(f&gt;0\\) is from one of the above sets, is \\(\\sqrt{f}\\) also from the same class? Some of the answers to the previous questions are negative. By manipulating or restricting parameters \\(L, \\alpha, \\beta\\), \\(A\\) and the images of functions develop versions of the above statements with affirmative answers. For example, if \\(f,g\\in \\mathcal{L}( L,\\mathbb{R},\\mathbb{R})\\) then \\(f \\circ g\\) might not be in \\(\\mathcal{L}( L,\\mathbb{R},\\mathbb{R})\\). However \\(f \\circ g \\in \\mathcal{L}( L^2,\\mathbb{R}, \\mathbb{R})\\). Think about why this is true. On a similar note, if \\(f,g\\in \\mathcal{L}( L,\\mathbb{R},[a,b])\\) for some positive \\(a&lt;b\\) then \\(f\\cdot g \\in \\mathcal{L}( 2Lb,\\mathbb{R},[a^2,b^2])\\) for some \\(a&#39;,b&#39;, L&#39;\\). Exercise 1.2 (Gradient descent) Let \\(f(x,y)=2x + e^{2x} + y^2\\). Function \\(f\\) restricted to \\(K=[-3,3]\\times [-3,3]\\) is Lipschitz, smooth and strongly convex. Find some corresponding (preferably optimal) constants \\(L, \\alpha\\) and \\(\\beta\\) on \\(K\\). Prove that \\(f\\) is convex. Determine the optimal learning rate \\(\\gamma\\) for the gradient descent method for this function. Solution. First, we compute the gradient \\[\\nabla f = \\begin{bmatrix} 2 + 2e^{2x}\\\\ 2y \\end{bmatrix}\\] and from \\[||\\nabla f|| = \\sqrt{(2 + 2e^{2x})^2 + (2y)^2} \\leq \\sqrt{(2 + 2e^{6})^2 + 6^2} = L \\text{ on } D\\] it follows that \\(f\\) is Lipshitz. Next, we compute the Hessian \\[\\nabla f = \\begin{bmatrix} 4e^{2x} &amp; 0\\\\ 0 &amp; 2 \\end{bmatrix}.\\] The eigenvalues \\(4e^{2x}\\) and \\(2\\) are always positive, so \\(f\\) is convex. The minimum and maximum eigenvalues on \\(D\\) are \\(\\alpha = 4e^{-6}\\) and \\(\\beta = 4e^{6}\\), so \\(f\\) is \\(4e^{-6}\\)-strongly convex and \\(4e^{6}\\)-smooth, respectively. It follows that the optimal learning rate is \\[\\gamma = \\frac{2}{\\alpha + \\beta} = \\frac{2}{4e^{-6} + 4e^{6}}.\\] Exercise 1.3 (Gradient descent) Let \\(f(x,y)= (x+y)^2 + (x-2y-3)^2\\). Starting with \\(x_1=(0,1)\\): What is the minimal function value that can be achieved with one step of the gradient descent, i.e., find the minimum of \\(f(x_2)\\). How close to the actual minumum \\(x^*\\) of function \\(f\\) can we get with one step of the gradient descent, i.e., find the minimum of the distance from \\(x^*\\) to \\(x_2\\). Solution. First, we compute the gradient: \\[f(x, y) = x^2 + 2xy + y^2 + x^2+ 4y^2 + 9 - 4xy - 6x + 12y = 2x^2 + 5y^2 - 2xy - 6x + 12y + 9\\] and \\[\\nabla f = \\begin{bmatrix} 4x - 2y - 6\\\\ 10y - 2x + 12 \\end{bmatrix}.\\] The value of the gradient at \\(x_1 = (0, 1)\\) is \\(\\begin{bmatrix} -8\\\\ 22 \\end{bmatrix}\\), so \\[x_2(\\gamma) = x_1 - \\gamma \\nabla f(x_1) = \\begin{bmatrix} 0\\\\ 1 \\end{bmatrix} - \\gamma \\begin{bmatrix} -8\\\\ 22 \\end{bmatrix} = \\begin{bmatrix} 8\\gamma\\\\ 1 - 22\\gamma \\end{bmatrix}\\]. The function \\[g(\\gamma) = f(8\\gamma, 1 - 22\\gamma)\\] \\[= 128\\gamma^2 + 5 + 2420\\gamma^2 - 220\\gamma - 16\\gamma + 352 \\gamma^2 - 48\\gamma + 12 - 264\\gamma + 9\\] \\[= 2900\\gamma^2 - 548\\gamma + 26\\] is quadratic. Its derivative is \\(5800x - 548\\) and \\(g\\) is minimized at \\(\\gamma^* = \\frac{548}{5800} = \\frac{137}{1450}\\). Pluggin this into \\(g(\\gamma)\\), we get \\(\\frac{81}{725}\\) - the minimum function value that can be achieved with a single step of gradient descent. The global minimum of \\(f\\) is 0 at \\(x^* = (1, -1)\\). We get the closest to \\(x^*\\) at \\[\\gamma^* = \\arg\\min_\\gamma \\sqrt{(8\\gamma - 1)^2 + (1 - 22\\gamma - (-1))^2}\\] \\[= \\arg\\min_\\gamma (8\\gamma - 1)^2 + (1 - 22\\gamma - (-1))^2\\] \\[= \\arg\\min_\\gamma 548\\gamma^2 - 104\\gamma + 5.\\] The above quadratic is minimized at \\(\\gamma^* = \\frac{13}{137}\\) and the minimum achievable distance is \\(\\frac{9}{137}\\). 1.2 Practical problems Find the optimum of these functions using the methods and techniques we learned in this course: Exercise 1.4 (Minimization problem - the Ackley function) \\[f(x_0 \\cdots x_n) = -20 \\exp\\left(-0.2 \\sqrt{\\frac{1}{n} \\sum_{i=1}^n x_i^2}\\right) - \\exp\\left(\\frac{1}{n} \\sum_{i=1}^n cos(2\\pi x_i)\\right) + 20 + e\\] \\[-32 \\leq x_i \\leq 32\\] \\[\\text{minimum at }f(0, \\cdots, 0) = 0\\] Exercise 1.5 (Minimization problem - an exponential function) \\[f(x_1 \\cdots x_n) = \\sum_{i=1}^n e^{ix_i}i + \\alpha\\] \\[\\alpha = -\\sum_{i=1}^n e^{-5.12i}\\] \\[-5.12 \\leq x_i \\leq 5.12\\] \\[\\text{minimum at }f(-5.12, \\cdots, -5.12) = 0\\] Exercise 1.6 (Minimization problem - the Griewank function) \\[f(x_1 \\cdots x_n) = 1 + \\frac{1}{4000} \\sum_{i=1}^n x^2_i - \\prod_{i=1}^n cos(\\frac{x_i}{\\sqrt{i}})\\] \\[-512 \\leq x_i \\leq 512\\] \\[\\text{minimum at }f(0, \\cdots, 0) = 0\\] Exercise 1.7 (Minimization problem - the Ridge function) \\[f(x_1 \\cdots x_n) = \\sum_{i=1}^n \\left(\\sum_{j=1}^i x_j\\right)^2\\] \\[-64 \\leq x_i \\leq 64\\] \\[\\text{minimum at }f(0, \\cdots, 0) = 0\\] Exercise 1.8 (Minimization problem - the Rosenbrock function) \\[f(x_1 \\cdots x_n) = \\sum_{i=1}^{n-1} (100(x_i^2 - x_{i+1})^2 + (1-x_i)^2)\\] \\[-2.048 \\leq x_i \\leq 2.048\\] \\[\\text{minimum at }f(1, 1, \\cdots, 1) = 0\\] Exercise 1.9 (Minimization problem - the Whitley function) \\[f(x_1 \\cdots x_n) = \\sum_{i=1}^n \\sum_{j=1}^n (\\frac{(100(x_i^2-x_j)^2 + (1-x_j)^2)^2}{4000} -\\cos(100(x_i^2-x_j)^2 + (1-x_j)^2)+1)\\] \\[-10.24 \\leq x_i \\leq 10.24\\] \\[\\text{minimum at }f(1, 1, \\cdots, 1) = 0\\] Exercise 1.10 (Minimization problem - the Matyas function) \\[ f(x,y)= .26 (x^2 + y^2) - .48 xy \\] \\[ -10 \\leq x,y \\leq 10 \\] \\[\\text{minimum at }f(0, 0) = 0\\] Exercise 1.11 (Minimization problem - the McCormick function) \\[ f(x,y)= \\sin(x+y)+(x-y)^2 - 1.5 x + 2.5 y + 1 \\] \\[ x\\in[-1.5,4], y\\in[-3.4] \\] \\[\\text{minimum at }f(-.54719, -1.54719) = -1.9133\\] Exercise 1.12 (Minimization problem - the Three-Hump Camel function) \\[ f(x,y)= 2x^2 - 1.05 x^4 + x^6/6 + xy + y^2 \\] \\[ -5 \\leq x,y \\leq 5 \\] \\[\\text{minimum at }f(0,0) = 0\\] "],
["markov.html", "Chapter 2 Markov Chains", " Chapter 2 Markov Chains .fold-btn { float: right; margin: 5px 5px 0 0; } .fold { border: 1px solid black; min-height: 40px; } "],
["MCMC.html", "Chapter 3 Markov Chain Monte Carlo", " Chapter 3 Markov Chain Monte Carlo .fold-btn { float: right; margin: 5px 5px 0 0; } .fold { border: 1px solid black; min-height: 40px; } "],
["references.html", "References", " References "]
]
